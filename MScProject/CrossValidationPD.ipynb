{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CrossValidationPD.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO+A4BI+FZb336aeQutgcjg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kingy434/Sam-portfolio/blob/main/MScProject/CrossValidationPD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "UwjoyJg73X3s",
        "outputId": "4a3913c9-01aa-4c63-b54c-b44b43425a53"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-06fa61f7-0ed1-4f2a-a651-5be7c24feb97\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-06fa61f7-0ed1-4f2a-a651-5be7c24feb97\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Tremor_matrix_with_causal_factors_and_prototypes.mat to Tremor_matrix_with_causal_factors_and_prototypes.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdxUyrrx0hT5"
      },
      "source": [
        "After loading in the mat file to Colab, we then make use of scipy.io to open the mat file and create a variable for the Y features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh78JEOfKS0V"
      },
      "source": [
        "from os.path import dirname, join as pjoin\n",
        "import scipy.io as sio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "mat_data = 'Tremor_matrix_with_causal_factors_and_prototypes.mat'\n",
        "mat_contents = sio.loadmat(mat_data)\n",
        "Y_features = mat_contents['Y_tremor_features_norm'];"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g0plci4G2B2"
      },
      "source": [
        "Before loading the features into a dataframe and organising them, I first define the relevant functions I will apply to the dataframe. To begin with, for LOOCV I found it convenient to define a losses function. For the kth iteration, this function takes k as the testing set and the rest as the training set and returns the error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACu-4aH3nCDq"
      },
      "source": [
        "def losses(data, algo, k, column):\n",
        "  #split into 2 dataframes with k and without it\n",
        "  data_filtered = data[data[column] != k]\n",
        "  data_cv = data[data[column] == k]\n",
        "   \n",
        "  # Test data - the person left out of training\n",
        "  data_test = data_cv.drop(columns=column)\n",
        "  X_test = data_test.drop(columns=[outcomevar])\n",
        "  y_test = data_test[outcomevar] #This is the outcome variable\n",
        "      \n",
        "  # Train data - all other people in dataframe\n",
        "  data_train = data_filtered.drop(columns=column)\n",
        "  X_train = data_train.drop(columns=[outcomevar])\n",
        "      \n",
        "  X_train = np.array(X_train)\n",
        "  y_train = np.array(data_train[outcomevar]) #Outcome variable here\n",
        "\n",
        "  # Train the model on training data\n",
        "  algo.fit(X_train, y_train);\n",
        "  predictions = algo.predict(X_test)\n",
        "  return abs(predictions - y_test.values)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fab5eQIrHcu6"
      },
      "source": [
        "The following code is an adaptation for Python of Bates' R code shown here: https://github.com/stephenbates19/nestedcv/blob/master/R/core.R. It is based off the algorithm detailed in Bates, Hastie, and Tibshinari's https://arxiv.org/pdf/2104.00673.pdf. I had to make some changes for LOOCV as the code relied on making K equal partitions for each fold, and with the case of using IDs there are not 8 equal partitions for each patient. I opted to generalise this for a specific column so I could experiment with nesting on things other than the ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFV3wYo6BTFs"
      },
      "source": [
        "from scipy import stats\n",
        "\n",
        "def naive_loocv(data, algo, column, alpha = 0.1):\n",
        "  \"\"\" Naive LOOCV\n",
        "  Performs Naive Leave One Out Cross Validation on data provided\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - column The features who's unique values will be used as partitions\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  Outputs:\n",
        "  - err_hat Mean of CV prediction errors\n",
        "  - ci_lo Lower endpoint for CV confidence interval on prediction error \n",
        "  - ci_hi Higher endpoint for CV confidence interval on prediction error\n",
        "  - sd Standard Deviation of the CV prediction errors\n",
        "  \"\"\"\n",
        "\n",
        "  errors = np.array([])\n",
        "  \n",
        "  #get list for the values to iterate over\n",
        "  vals = data[column].unique()\n",
        "  for k in vals:\n",
        "    #find error with k as testing subject\n",
        "    error_k = losses(data, algo, k, column)\n",
        "    errors = np.concatenate((errors, error_k))\n",
        "\n",
        "    print(\"...\" + str(int(k)) + \" processing complete\")\n",
        "\n",
        "  #mean error\n",
        "  err_hat = np.mean(errors)\n",
        "  \n",
        "  #defines the low and high confidence intervals for the error\n",
        "  ci_lo = np.mean(errors) - stats.norm.ppf(1-alpha/2) * np.std(errors) / np.sqrt(len(data.index))\n",
        "  ci_hi = np.mean(errors) + stats.norm.ppf(1-alpha/2) * np.std(errors) / np.sqrt(len(data.index))\n",
        "\n",
        "  #standard deviation of errors\n",
        "  sd = np.std(errors)\n",
        "\n",
        "  #dictionary returned of relevant values\n",
        "  return {'err_hat': err_hat, 'ci_lo': ci_lo, 'ci_hi': ci_hi, \"sd\": sd}"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxPjDULPYwVE"
      },
      "source": [
        "def nested_loocv_helper(data, algo, column, alpha = 0.1):\n",
        "  \"\"\" Nested LOOCV helper\n",
        "  Function to aid Nested LOOCV. Similar to Naive but with additional nesting stage\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - column The features who's unique values will be used as partitions\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  Outputs:\n",
        "  - out-mat Vector of difference statistics with same length as target\n",
        "  - all_ho_errors Vector of hold-out errors not used in model training\n",
        "  \"\"\"\n",
        "\n",
        "  #define number of folds needed for column \n",
        "  vals = data[column].unique()\n",
        "  K = len(vals)\n",
        "\n",
        "  #fold 1 iterates through K-1 points, and fold 2 iterates through remainder of list after fold 1\n",
        "  ho_errors = {}\n",
        "  #entry i,j is error on fold i when folds i & j are not used for model fitting\n",
        "  for f1 in range(K-1):\n",
        "    for f2 in range(f1+1, K):\n",
        "      ho_errors[(f1,f2)] = losses(data, algo, vals[f1], column)\n",
        "      ho_errors[(f2,f1)] = losses(data[data[column] != vals[f1]], algo, vals[f2], column)\n",
        "\n",
        "  #loops over all points in the two folds, except when the values are equal\n",
        "  out_mat = np.zeros((K, 2))\n",
        "  for f1 in range(K):\n",
        "    e_out = losses(data, algo, vals[f1], column)\n",
        "\n",
        "    e_bar_t = np.array([])\n",
        "    for f2 in range(K):\n",
        "      if f1 != f2:\n",
        "        e_bar_t = np.concatenate((e_bar_t, ho_errors[(f2,f1)]))\n",
        "\n",
        "    out_mat[f1,0] = np.mean(e_bar_t) - np.mean(e_out)\n",
        "    out_mat[f1,1] = np.var(e_out) / len(data[data[column] == vals[f1]].index)\n",
        "\n",
        "  all_ho_errors = np.array([])\n",
        "  #produces final hold-out error vector for the values computed\n",
        "  for f1 in range(K-1):\n",
        "    for f2 in range(f1+1, K):\n",
        "      all_ho_errors = np.concatenate((all_ho_errors, ho_errors[(f1,f2)], ho_errors[(f2,f1)]))\n",
        "\n",
        "  #returns dictionary with outputs\n",
        "  return {\"pivots\": out_mat, \"errs\": all_ho_errors}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBBtu7Nhr6Wi"
      },
      "source": [
        "import datetime\n",
        "import math\n",
        "\n",
        "def nested_loocv(data, algo, column, reps=50, alpha=0.1, bias_reps=None, time=False):\n",
        "  \"\"\" Nested LOOCV\n",
        "  Performs Nested LOOCV over several reps and computes bias\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - column The features who's unique values will be used as partitions\n",
        "  - reps The number of times the algorithm should be repeated, the higher the more accurate it is\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  - bias_reps The number of repetitions used to calculate the bias\n",
        "  - time Useful for several repetitions to get an estimate of how long it'll take to run\n",
        "  Outputs:\n",
        "  - sd_infl The multiplicative factor needed to correct the interval width\n",
        "  - err_hat Mean of CV prediction erorrs corrected for bias\n",
        "  - ci_lo Lower endpoint for CV confidence interval on prediction error \n",
        "  - ci_hi Higher endpoint for CV confidence interval on prediction error\n",
        "  - raw_mean Mean of CV prediction errors without correction for bias\n",
        "  - bias_est Estimation of bias from Naive LOOCV\n",
        "  - sd Standard deviation of the prediction errors\n",
        "  - running_sd_infl Used to evaluate how stable the estimate is and if more replicates are needed\n",
        "  \"\"\"\n",
        "\n",
        "  #number of folds\n",
        "  K = len(data[column].unique())\n",
        "\n",
        "  #time estimation\n",
        "  if time == True:\n",
        "    t1 = datetime.datetime.now()\n",
        "    temp = nested_loocv_helper(data, algo, column)\n",
        "    t2 = datetime.datetime.now()\n",
        "    print(\"Estimated time required: {}\".format((t2-t1)*reps))\n",
        "\n",
        "  #runs the algorithm several time appending the results for each repetition\n",
        "  var_pivots = np.zeros((reps*K, 2))\n",
        "  ho_errs = np.array([])\n",
        "  for i in range(reps):\n",
        "    var_pivots[K*i:K*(i+1)] = nested_loocv_helper(data, algo, column)[\"pivots\"]\n",
        "    ho_errs = np.concatenate((ho_errs, nested_loocv_helper(data, algo, column)[\"errs\"]))\n",
        "    print(\"...{} rep completed!\".format(i))\n",
        "\n",
        "  n_sub = math.floor((len(data[column])) * (K-1) / K)\n",
        "  #estimation of inflation after each repetition\n",
        "  ugp_infl = np.sqrt(max(0, np.mean(var_pivots[:,0]**2 - var_pivots[:,1]))) / (np.std(ho_errs) / np.sqrt(n_sub))\n",
        "  ugp_infl = max(1, min(ugp_infl, np.sqrt(K)))\n",
        "\n",
        "  #estimate of inflation at each time step\n",
        "  infl_est2 = np.sqrt(np.maximum(0, np.array([np.mean(var_pivots[:(i+1)*K,0]**2 - var_pivots[:(i+1)*K, 1]) for i in range(reps)]))) / (np.std(ho_errs) / np.sqrt(n_sub))\n",
        "\n",
        "  #calculates the bias using Naive LOOCV result\n",
        "  cv_means = np.array([])\n",
        "  bias_est = 0\n",
        "  if bias_reps is None:\n",
        "    bias_reps = math.ceil(reps / 5)\n",
        "  if bias_reps == 0:\n",
        "    bias_est = 0\n",
        "  else:\n",
        "    for i in range(bias_reps):\n",
        "      temp = naive_loocv(data, algo, column)\n",
        "      cv_means = np.append(cv_means, temp[\"err_hat\"])\n",
        "      print(\"...{} bias rep completed!\".format(i))\n",
        "\n",
        "    bias_est = (np.mean(ho_errs) - np.mean(cv_means)) * (1 + ((K - 2) / (K))**(1.5))\n",
        "\n",
        "  pred_est = np.mean(ho_errs) - bias_est\n",
        "  ci_lo = pred_est - stats.norm.ppf(1-alpha/2) * np.std(ho_errs) / np.sqrt(len(data[column])) * ugp_infl\n",
        "  ci_hi = pred_est + stats.norm.ppf(1-alpha/2) * np.std(ho_errs) / np.sqrt(len(data[column])) * ugp_infl\n",
        "\n",
        "  return {\"sd_infl\": ugp_infl, \"err_hat\": pred_est, \"ci_lo\": ci_lo, \"ci_hi\": ci_hi, \"raw_mean\": np.mean(ho_errs), \"bias_est\": bias_est, \"sd\": np.std(ho_errs), \"running_sd_inf1\": infl_est2}"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsPjEuhgSSif"
      },
      "source": [
        "def naive_tenfoldcv(data, algo, alpha=0.1):\n",
        "  \"\"\" Naive 10-fold CV\n",
        "  Performs Naive 10-fold Cross Validation on data provided\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  Outputs:\n",
        "  - err_hat Mean of CV prediction errors\n",
        "  - ci_lo Lower endpoint for CV confidence interval on prediction error \n",
        "  - ci_hi Higher endpoint for CV confidence interval on prediction error\n",
        "  - sd Standard Deviation of the CV prediction errors\n",
        "  \"\"\"\n",
        "\n",
        "  X = data.drop(columns=[outcomevar]).to_numpy()\n",
        "  y = data[outcomevar].to_numpy()\n",
        "\n",
        "  fold_id = np.array([i % 10 for i in range(len(data.index))])\n",
        "  np.random.shuffle(fold_id)\n",
        "\n",
        "  errors = np.array([])\n",
        "  for k in range(10):\n",
        "    algo.fit(X[fold_id != k], y[fold_id != k])\n",
        "    predictions = algo.predict(X[fold_id == k])\n",
        "    error_k = abs(predictions - y[fold_id == k])\n",
        "    errors = np.concatenate((errors, error_k))\n",
        "\n",
        "  err_hat = np.mean(errors)\n",
        "  ci_lo = np.mean(errors) - stats.norm.ppf(1-alpha/2) * np.std(errors) / np.sqrt(len(y))\n",
        "  ci_hi = np.mean(errors) + stats.norm.ppf(1-alpha/2) * np.std(errors) / np.sqrt(len(y))\n",
        "  sd = np.std(errors)\n",
        "\n",
        "  return {'err_hat': err_hat, 'ci_lo': ci_lo, 'ci_hi': ci_hi, \"sd\": sd}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N17HonN3ncDB"
      },
      "source": [
        "def nested_tenfoldcv_helper(data, algo):\n",
        "  \"\"\" Nested 10-fold CV helper\n",
        "  Function to aid Nested 10-fold CV. Similar to Naive but with additional nesting stage\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  Outputs:\n",
        "  - out-mat Vector of difference statistics with same length as target\n",
        "  - all_ho_errors Vector of hold-out errors not used in model training\n",
        "  \"\"\"\n",
        "\n",
        "  X = data.drop(columns=[outcomevar]).to_numpy()\n",
        "  y = data[outcomevar].to_numpy()\n",
        "\n",
        "  fold_id = np.array([i % 10 for i in range(len(y) // 10 * 10)])\n",
        "  np.random.shuffle(fold_id)\n",
        "  fold_id = np.concatenate((fold_id, np.repeat(10, len(y) % 10)))\n",
        "\n",
        "  ho_errors = np.zeros((10, 10, len(y) // 10))\n",
        "  for f1 in range(9):\n",
        "    for f2 in range(f1+1, 10):\n",
        "      test_idx = np.append(np.where(fold_id == f1), np.where(fold_id == f2))\n",
        "      algo.fit(np.delete(X, test_idx, 0), np.delete(y, test_idx, 0))\n",
        "      preds = algo.predict(X)\n",
        "      ho_errors[f1,f2] = abs(preds[fold_id == f1] - y[fold_id == f1])\n",
        "      ho_errors[f2,f1] = abs(preds[fold_id == f2] - y[fold_id == f2])\n",
        "\n",
        "  out_mat = np.zeros((10, 2))\n",
        "  for f1 in range(10):\n",
        "    test_idx = np.where(fold_id == f1)\n",
        "    algo.fit(np.delete(X, test_idx, 0), np.delete(y, test_idx, 0))\n",
        "    preds = algo.predict(X[test_idx])\n",
        "    e_out = abs(preds - y[test_idx])\n",
        "\n",
        "    e_bar_t = np.array([])\n",
        "    for f2 in range(10):\n",
        "      if f1 != f2:\n",
        "        e_bar_t = np.concatenate((e_bar_t, ho_errors[f2,f1]))\n",
        "\n",
        "    out_mat[f1,0] = np.mean(e_bar_t) - np.mean(e_out)\n",
        "    out_mat[f1,1] = np.var(e_out) / len(test_idx)\n",
        "\n",
        "  all_ho_errors = np.array([])\n",
        "  for f1 in range(9):\n",
        "    for f2 in range(f1+1, 10):\n",
        "      all_ho_errors = np.concatenate((all_ho_errors, ho_errors[f1,f2], ho_errors[f2,f1]))\n",
        "\n",
        "  return {\"pivots\": out_mat, \"errs\": all_ho_errors}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96ZmK_aA0n3U"
      },
      "source": [
        "def nested_tenfoldcv(data, algo, reps=50, alpha=0.1, bias_reps=None, time=False):\n",
        "    \"\"\" Nested 10-fold CV\n",
        "  Performs Nested 10-fold CV over several reps and computes bias\n",
        "  Inputs:\n",
        "  - data A dataframe of features and response variable\n",
        "  - algo A classification algorithm object, with the attributes fit and predict\n",
        "  - reps The number of times the algorithm should be repeated, the higher the more accurate it is\n",
        "  - alpha The error rate. Must be in (0, 0.5)\n",
        "  - bias_reps The number of repetitions used to calculate the bias\n",
        "  - time Useful for several repetitions to get an estimate of how long it'll take to run\n",
        "  Outputs:\n",
        "  - sd_infl The multiplicative factor needed to correct the interval width\n",
        "  - err_hat Mean of CV prediction erorrs corrected for bias\n",
        "  - ci_lo Lower endpoint for CV confidence interval on prediction error \n",
        "  - ci_hi Higher endpoint for CV confidence interval on prediction error\n",
        "  - raw_mean Mean of CV prediction errors without correction for bias\n",
        "  - bias_est Estimation of bias from Naive LOOCV\n",
        "  - sd Standard deviation of the prediction errors\n",
        "  - running_sd_infl Used to evaluate how stable the estimate is and if more replicates are needed\n",
        "  \"\"\"\n",
        "\n",
        "  X = data.drop(columns=[outcomevar]).to_numpy()\n",
        "  y = data[outcomevar].to_numpy()\n",
        "\n",
        "  if time == True:\n",
        "    t1 = datetime.datetime.now()\n",
        "    temp = nested_tenfoldcv_helper(data, algo)\n",
        "    t2 = datetime.datetime.now()\n",
        "    print(\"Estimated time required: {}\".format((t2-t1)*reps))\n",
        "\n",
        "  var_pivots = np.zeros((reps*10, 2))\n",
        "  ho_errs = np.array([])\n",
        "  for i in range(reps):\n",
        "    var_pivots[10*i:10*(i+1)] = nested_tenfoldcv_helper(data, algo)[\"pivots\"]\n",
        "    ho_errs = np.concatenate((ho_errs, nested_tenfoldcv_helper(data, algo)[\"errs\"]))\n",
        "    print(\"...{} rep completed!\".format(i))\n",
        "\n",
        "  n_sub = math.floor(len(y) * 9 / 10)\n",
        "  ugp_infl = np.sqrt(max(0, np.mean(var_pivots[:,0]**2 - var_pivots[:,1]))) / (np.std(ho_errs) / np.sqrt(n_sub))\n",
        "  ugp_infl = max(1, min(ugp_infl, np.sqrt(10)))\n",
        "\n",
        "  infl_est2 = np.sqrt(np.maximum(0, np.array([np.mean(var_pivots[:(i+1)*10,0]**2 - var_pivots[:(i+1)*10, 1]) for i in range(reps)]))) / (np.std(ho_errs) / np.sqrt(n_sub))\n",
        "\n",
        "  cv_means = np.array([])\n",
        "  bias_est = 0\n",
        "  if bias_reps is None:\n",
        "    bias_reps = math.ceil(reps / 5)\n",
        "  if bias_reps == 0:\n",
        "    bias_est = 0\n",
        "  else:\n",
        "    for i in range(bias_reps):\n",
        "      temp = naive_tenfoldcv(data, algo)\n",
        "      cv_means = np.append(cv_means, temp[\"err_hat\"])\n",
        "      print(\"...{} bias rep completed!\".format(i))\n",
        "\n",
        "    bias_est = (np.mean(ho_errs) - np.mean(cv_means)) * (1 + 0.8**1.5)\n",
        "\n",
        "  pred_est = np.mean(ho_errs) - bias_est\n",
        "  ci_lo = pred_est - stats.norm.ppf(1-alpha/2) * np.std(ho_errs) / np.sqrt(len(y)) * ugp_infl\n",
        "  ci_hi = pred_est + stats.norm.ppf(1-alpha/2) * np.std(ho_errs) / np.sqrt(len(y)) * ugp_infl\n",
        "\n",
        "  return {\"sd_inf1\": ugp_infl, \"err_hat\": pred_est, \"ci_lo\": ci_lo, \"ci_hi\": ci_hi, \"raw_mean\": np.mean(ho_errs), \"bias_est\": bias_est, \"sd\": np.std(ho_errs), \"running_sd_inf1\": infl_est2}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJCFEaGGR2Lf"
      },
      "source": [
        "Y_features_whiten = Y_features[:,1:46]\n",
        "Y_features_whiten = Y_features_whiten - np.mean(Y_features_whiten,axis=0)\n",
        "from sklearn.decomposition import PCA\n",
        "for i in range(45):\n",
        "  pca = PCA(n_components=i+1)\n",
        "  pca.fit(Y_features_whiten)\n",
        "  if np.cumsum(pca.explained_variance_ratio_)[-1] > 0.95:\n",
        "    opt_components = i+1\n",
        "    break\n",
        "Y_features_whiten = pca.fit_transform(Y_features_whiten)\n",
        "Y_features_final = np.zeros((len(Y_features[:,0]), 28))\n",
        "Y_features_final[:,0] = Y_features[:,0]\n",
        "Y_features_final[:,1:opt_components+1] = Y_features_whiten\n",
        "Y_features_final[:,opt_components+1:] = Y_features[:,46:]\n",
        "\n",
        "data_features = pd.DataFrame(data=Y_features_final, columns=[\"ID\", \"Col2\", \"Col3\", \"Col4\", \"Col5\", \"Col6\", \"Col7\", \"Col8\", \"Col9\", \"Col10\",\"Col11\", \"Col12\", \"Col13\", \"Col14\", \"Col15\", \"Col16\", \"Col17\", \"Col18\", \"Col19\", \"Col20\",\"Col21\", \"Col22\", \"Col23\", \"Col24\", \"Medication_Intake\",\"Prototype_ID\",\"Non-tremor/Tremor\",\"Activity_label\"])\n",
        "\n",
        "data_features = data_features.dropna()\n",
        "data_features_id = data_features.drop(columns=[\"Prototype_ID\", \"Activity_label\", \"Medication_Intake\"])\n",
        "data_features_med = data_features.drop(columns=[\"Prototype_ID\", \"Activity_label\", \"ID\"])\n",
        "data_features_cols = data_features.drop(columns=[\"Prototype_ID\", \"Activity_label\", \"ID\", \"Medication_Intake\"])\n",
        "\n",
        "outcomevar = 'Non-tremor/Tremor'\n",
        "outcomevar_id = 48\n",
        "idcolumn = 'ID'\n",
        "idcolumn_id = 1\n",
        "medcolumn = 'Medication_Intake'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZsEV7czYmS8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fe63de8-7a02-4919-fd94-c97453be49ff"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf1 = RandomForestClassifier(n_estimators = 100, random_state = 0, class_weight=\"balanced\")\n",
        "clf2 = LogisticRegression(class_weight=\"balanced\", random_state=0, solver=\"newton-cg\")\n",
        "\n",
        "naive_loocv_id_logreg_results = naive_loocv(data_features_id, clf2, idcolumn)\n",
        "print(\"Results for Naive LOOCV using IDs as folds with a Logistic Regression Classifier:\\n {}\".format(naive_loocv_id_logreg_results))\n",
        "\n",
        "nested_loocv_id_logreg_results = nested_loocv(data_features_id, clf2, idcolumn, reps=5, bias_reps=2)\n",
        "print(\"\\nResults for Nested LOOCV using IDs as folds with a Logistic Regression Classifier:\\n {}\".format(nested_loocv_id_logreg_results))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...1 processing complete\n",
            "...2 processing complete\n",
            "...3 processing complete\n",
            "...4 processing complete\n",
            "...5 processing complete\n",
            "...6 processing complete\n",
            "...7 processing complete\n",
            "...8 processing complete\n",
            "Results for Naive LOOCV using IDs as folds with a Logistic Regression Classifier:\n",
            " {'err_hat': 0.18309559009602963, 'ci_lo': 0.18077399330364438, 'ci_hi': 0.18541718688841488, 'sd': 0.38674487066206364}\n",
            "...0 rep completed!\n",
            "...1 rep completed!\n",
            "...2 rep completed!\n",
            "...3 rep completed!\n",
            "...4 rep completed!\n",
            "...1 processing complete\n",
            "...2 processing complete\n",
            "...3 processing complete\n",
            "...4 processing complete\n",
            "...5 processing complete\n",
            "...6 processing complete\n",
            "...7 processing complete\n",
            "...8 processing complete\n",
            "...0 bias rep completed!\n",
            "...1 processing complete\n",
            "...2 processing complete\n",
            "...3 processing complete\n",
            "...4 processing complete\n",
            "...5 processing complete\n",
            "...6 processing complete\n",
            "...7 processing complete\n",
            "...8 processing complete\n",
            "...1 bias rep completed!\n",
            "\n",
            "Results for Nested LOOCV using IDs as folds with a Logistic Regression Classifier:\n",
            " {'sd_infl': 2.8284271247461903, 'err_hat': 0.1815668505283608, 'ci_lo': 0.17496783986062514, 'ci_hi': 0.18816586119609646, 'raw_mean': 0.18544923863180146, 'bias_est': 0.003882388103440668, 'sd': 0.3886615732519575, 'running_sd_inf1': array([57.53855126, 57.53855126, 57.53855126, 57.53855126, 57.53855126])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7TbTXnnMkiG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23a8e314-2d80-40ae-fa68-0de5bfb8abb6"
      },
      "source": [
        "naive_loocv_id_rf_results = naive_loocv(data_features_id, clf1, idcolumn)\n",
        "print(\"Results for Naive LOOCV using IDs as folds with a Random Forest Classifier:\\n {}\".format(naive_loocv_id_rf_results))\n",
        "\n",
        "nested_loocv_id_rf_results = nested_loocv(data_features_id, clf1, idcolumn, reps=1, bias_reps=1)\n",
        "print(\"\\nResults for Nested LOOCV using IDs as folds with a Random Forest Classifier:\\n {}\".format(nested_loocv_id_rf_results))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "...1 processing complete\n",
            "...2 processing complete\n",
            "...3 processing complete\n",
            "...4 processing complete\n",
            "...5 processing complete\n",
            "...6 processing complete\n",
            "...7 processing complete\n",
            "...8 processing complete\n",
            "Results for Naive LOOCV using IDs as folds with a Random Forest Classifier:\n",
            " {'err_hat': 0.10033164182682702, 'ci_lo': 0.09852811579449769, 'ci_hi': 0.10213516785915636, 'sd': 0.30044168065559795}\n",
            "...0 rep completed!\n",
            "...1 processing complete\n",
            "...2 processing complete\n",
            "...3 processing complete\n",
            "...4 processing complete\n",
            "...5 processing complete\n",
            "...6 processing complete\n",
            "...7 processing complete\n",
            "...8 processing complete\n",
            "...0 bias rep completed!\n",
            "\n",
            "Results for Nested LOOCV using IDs as folds with a Random Forest Classifier:\n",
            " {'sd_infl': 2.8284271247461903, 'err_hat': 0.10010918982829801, 'ci_lo': 0.09500032149350214, 'ci_hi': 0.10521805816309389, 'raw_mean': 0.10067412908344701, 'bias_est': 0.0005649392551490062, 'sd': 0.3008967411201664, 'running_sd_inf1': array([66.61239235])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xMqKFD2-lDTQ",
        "outputId": "ed175b99-aeae-416b-b382-9570ca1b2b12"
      },
      "source": [
        "naive_tenfoldcv_logreg_result = naive_tenfoldcv(data_features_cols, clf2)\n",
        "print(\"Results for Naive 10-fold CV with a Logistic Regression Classifier:\\n {}\".format(naive_tenfoldcv_logreg_result))\n",
        "\n",
        "nested_tenfoldcv_logreg_result = nested_tenfoldcv(data_features_cols, clf2, reps=5, bias_reps=2)\n",
        "print(\"\\nResults for Nested 10-fold CV with a Logistic Regression Classifier:\\n {}\".format(nested_tenfoldcv_logreg_result))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for Naive 10-fold CV with a Logistic Regression Classifier:\n",
            " {'err_hat': 0.1753839187011361, 'ci_lo': 0.17310103904777846, 'ci_hi': 0.17766679835449373, 'sd': 0.3802951482232838}\n",
            "...0 rep completed!\n",
            "...1 rep completed!\n",
            "...2 rep completed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:426: LineSearchWarning: Rounding errors prevent the line search from converging\n",
            "  warn(msg, LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/optimize/linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
            "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/optimize.py:204: UserWarning: Line Search failed\n",
            "  warnings.warn('Line Search failed')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "...3 rep completed!\n",
            "...4 rep completed!\n",
            "...0 bias rep completed!\n",
            "...1 bias rep completed!\n",
            "\n",
            "Results for Nested 10-fold CV with a Logistic Regression Classifier:\n",
            " {'sd_inf1': 1, 'err_hat': 0.1760213863409817, 'ci_lo': 0.1737355494586046, 'ci_hi': 0.1783072232233588, 'raw_mean': 0.17596193689693956, 'bias_est': -5.944944404213528e-05, 'sd': 0.380787780345453, 'running_sd_inf1': array([0., 0., 0., 0., 0.])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mqy5WGTjfV9",
        "outputId": "fcc244a3-01c0-433e-b73e-1c7ce2512049"
      },
      "source": [
        "naive_tenfoldcv_rf_result = naive_tenfoldcv(data_features_cols, clf1)\n",
        "print(\"Results for Naive 10-fold CV with a Random Forest Classifier:\\n {}\".format(naive_tenfoldcv_rf_result))\n",
        "\n",
        "nested_tenfoldcv_rf_result = nested_tenfoldcv(data_features_cols, clf1, reps=1, bias_reps=1)\n",
        "print(\"\\nResults for Nested 10-fold CV with a Random Forest Classifier:\\n {}\".format(nested_tenfoldcv_rf_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results for Naive 10-fold CV with a Random Forest Classifier:\n",
            " {'err_hat': 0.06763362235452378, 'ci_lo': 0.06612619337992215, 'ci_hi': 0.0691410513291254, 'sd': 0.25111613942900896}\n",
            "Estimated time required: 0:30:39.534541\n",
            "...0 rep completed!\n",
            "...0 bias rep completed!\n",
            "\n",
            "Results for Nested 10-fold CV with a Random Forest Classifier:\n",
            " {'sd_inf1': 1, 'err_hat': 0.0679084830196137, 'ci_lo': 0.06639579525407753, 'ci_hi': 0.06942117078514988, 'raw_mean': 0.06814360977919849, 'bias_est': 0.0002351267595847864, 'sd': 0.2519921788973991, 'running_sd_inf1': array([0.])}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}